{"cells":[{"cell_type":"code","source":["df = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/conddition_era_csv_1.gz')\n    \ndf.printSchema()    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2150940e-4a3a-464c-99d2-b11fabde2ef5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = df.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cfd1ca6-c273-467f-84c8-3431020beba8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sql\n\n# CREATE SCHEMA pankaj_gt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"008df866-b0e4-4b63-9908-d7cc28d33638"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.analysis.NamespaceAlreadyExistsException: Namespace 'pankaj_gt' already exists\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.namespaceAlreadyExistsError(QueryCompilationErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:53)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:189)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:336)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:148)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:286)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:687)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:549)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:215)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:95)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:213)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:210)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:251)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:243)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:549)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:215)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:95)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:213)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:210)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:251)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:243)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: NamespaceAlreadyExistsException: Namespace 'pankaj_gt' already exists","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.analysis.NamespaceAlreadyExistsException: Namespace 'pankaj_gt' already exists\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.namespaceAlreadyExistsError(QueryCompilationErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:53)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:189)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:336)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:148)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:286)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:140)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:687)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:549)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:215)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:95)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:213)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:210)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:251)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:243)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:549)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:215)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:95)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:213)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:210)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:251)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:243)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS pankaj_gt.condition_era1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a26709-9a29-4d61-81df-b910fd7831a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.write.saveAsTable(\"pankaj_gt.condition_era1\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1e3b1d5-97ca-4a14-b5cf-ccf3cc8bb0c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2 = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/condition_era_csv_2.gz')\ndf2.printSchema()    \ndf2 = df2.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')\ndf2.write.saveAsTable(\"pankaj_gt.condition_era2\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37aedc9a-575a-4ee5-a2fa-5921f5bf9b15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df3 = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/condition_era_csv_3.gz')\ndf3.printSchema()    \ndf3 = df3.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')\ndf3.write.saveAsTable(\"pankaj_gt.condition_era3\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ca8d6b5-2466-4cc4-a6a8-7c1f0a3298f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df4 = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/condition_era_csv_4.gz')\ndf4.printSchema()    \ndf4 = df4.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')\ndf4.write.saveAsTable(\"pankaj_gt.condition_era4\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4610c26-c8cd-42f0-ae9b-84147ff857e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df5 = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/condition_era_csv_5.gz')\ndf5.printSchema()    \ndf5 = df5.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')\ndf5.write.saveAsTable(\"pankaj_gt.condition_era5\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b07471f-3f80-4151-860d-4556e4e0fac8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df6 = spark.read.format('csv').options(header='false', inferSchema='true').load('/FileStore/pankaj/gatech/condition_era_csv_6.gz')\ndf6.printSchema()    \ndf6 = df6.toDF('condition_era_id','person_id','condition_concept_id','condition_era_start_date','condition_era_end_date','condition_occurrence_count')\ndf6.write.saveAsTable(\"pankaj_gt.condition_era6\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86fcbcd1-1211-429f-841a-c541747f6817"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: timestamp (nullable = true)\n |-- _c4: timestamp (nullable = true)\n |-- _c5: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- _c1: integer (nullable = true)\n-- _c2: integer (nullable = true)\n-- _c3: timestamp (nullable = true)\n-- _c4: timestamp (nullable = true)\n-- _c5: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\ncreate table pankaj_gt.condition_era as select * from pankaj_gt.condition_era1;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7656cb-106b-4cf0-b804-5e049fe449c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\ninsert into pankaj_gt.condition_era select * from pankaj_gt.condition_era2;\ninsert into pankaj_gt.condition_era select * from pankaj_gt.condition_era3;\ninsert into pankaj_gt.condition_era select * from pankaj_gt.condition_era4;\ninsert into pankaj_gt.condition_era select * from pankaj_gt.condition_era5;\ninsert into pankaj_gt.condition_era select * from pankaj_gt.condition_era6;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41628cf7-5eeb-4f16-b4a5-74056143ed84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[37157997,37157997]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>37157997</td><td>37157997</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\nselect 37376476+\n37232905+\n37225190+\n37183277+\n37217491+\n37157997;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69404777-1288-4c50-a1e4-74420d092292"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[223393336]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"(((((37376476 + 37232905) + 37225190) + 37183277) + 37217491) + 37157997)","type":"\"integer\"","metadata":"{\"__autoGeneratedAlias\":\"true\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>(((((37376476 + 37232905) + 37225190) + 37183277) + 37217491) + 37157997)</th></tr></thead><tbody><tr><td>223393336</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"load_claims_data_condition_era","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2521304938251248}},"nbformat":4,"nbformat_minor":0}
